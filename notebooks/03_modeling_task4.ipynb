{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52c46a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost: True shap: True\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# Task-4: Memory-safe notebook\n",
    "# ===========================\n",
    "\n",
    "# Cell 0 — imports & settings\n",
    "import os, json, warnings\n",
    "import numpy as np, pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "sns.set()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, SGDRegressor\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error, r2_score, mean_absolute_error,\n",
    "    roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    ")\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Optional: XGBoost + SHAP\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import shap\n",
    "    HAS_SHAP = True\n",
    "except Exception:\n",
    "    HAS_SHAP = False\n",
    "\n",
    "# Create folders\n",
    "os.makedirs(\"reports/interim/figures\", exist_ok=True)\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "print(\"xgboost:\", HAS_XGB, \"shap:\", HAS_SHAP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d0f6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ../data/raw/merged_dataset.csv shape: (1000099, 55)\n",
      "Columns: 55\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 — robust loader (edit paths if needed)\n",
    "candidates = [\n",
    "    \"../data/raw/merged_dataset.csv\",\n",
    "    \"merged_dataset.csv\",\n",
    "    \"task3_merged_dataset.csv\",\n",
    "    \"data/raw/insurance_data.txt\",\n",
    "    \"data.csv\"\n",
    "]\n",
    "df = None\n",
    "for p in candidates:\n",
    "    if os.path.exists(p):\n",
    "        try:\n",
    "            df = pd.read_csv(p, low_memory=True)\n",
    "            print(\"Loaded:\", p, \"shape:\", df.shape)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"Found but could not read:\", p, e)\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\"No merged_dataset found. Place merged_dataset.csv in notebook folder.\")\n",
    "\n",
    "# Basic cleaning & numeric conversions\n",
    "df.columns = df.columns.str.strip()\n",
    "for col in [\"TotalClaims\",\"TotalPremium\",\"CalculatedPremiumPerTerm\",\"CustomValueEstimate\"]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col].astype(str).str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True), errors='coerce')\n",
    "\n",
    "# derive common fields\n",
    "if \"ClaimEvent\" not in df.columns and \"TotalClaims\" in df.columns:\n",
    "    df[\"ClaimEvent\"] = (df[\"TotalClaims\"].fillna(0).astype(float) > 0).astype(int)\n",
    "if \"Margin\" not in df.columns and \"TotalPremium\" in df.columns and \"TotalClaims\" in df.columns:\n",
    "    df[\"Margin\"] = df[\"TotalPremium\"].fillna(0) - df[\"TotalClaims\"].fillna(0)\n",
    "if \"Severity\" not in df.columns:\n",
    "    df[\"Severity\"] = df[\"TotalClaims\"].where(df.get(\"ClaimEvent\",0)==1)\n",
    "\n",
    "print(\"Columns:\", len(df.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c89003a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate features: ['vehicle_age', 'make', 'VehicleType', 'Gender', 'Province', 'PostalCode', 'TermFrequency', 'AlarmImmobiliser', 'TrackingDevice', 'CustomValueEstimate', 'Cylinders', 'kilowatts', 'bodytype', 'NumberOfDoors', 'NumberOfVehiclesInFleet', 'SumInsured', 'RegistrationYear']\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 — vectorized feature engineering (fast)\n",
    "d = df.copy()\n",
    "\n",
    "# TransactionMonth -> year\n",
    "if \"TransactionMonth\" in d.columns:\n",
    "    d[\"TransactionMonth\"] = pd.to_datetime(d[\"TransactionMonth\"], errors=\"coerce\")\n",
    "    d[\"transaction_year\"] = d[\"TransactionMonth\"].dt.year\n",
    "\n",
    "# vehicle_age (vectorized)\n",
    "if \"RegistrationYear\" in d.columns:\n",
    "    d[\"RegistrationYear\"] = pd.to_numeric(d[\"RegistrationYear\"], errors=\"coerce\")\n",
    "    d[\"vehicle_age\"] = d[\"transaction_year\"] - d[\"RegistrationYear\"]\n",
    "\n",
    "# normalize text columns (lowercase, strip)\n",
    "text_cols = [\"make\",\"Model\",\"VehicleType\",\"Gender\",\"Province\",\"PostalCode\",\"CoverType\",\"Product\"]\n",
    "for c in text_cols:\n",
    "    if c in d.columns:\n",
    "        d[c] = d[c].astype(str).str.strip().str.lower().replace(\"nan\", np.nan)\n",
    "\n",
    "# Candidate features: adjust as needed\n",
    "candidate_features = [c for c in [\n",
    "    \"vehicle_age\",\"make\",\"VehicleType\",\"Gender\",\"Province\",\"PostalCode\",\n",
    "    \"TermFrequency\",\"AlarmImmobiliser\",\"TrackingDevice\",\"CustomValueEstimate\",\n",
    "    \"Cylinders\",\"kilowatts\",\"bodytype\",\"NumberOfDoors\",\"NumberOfVehiclesInFleet\",\n",
    "    \"SumInsured\",\"RegistrationYear\"\n",
    "] if c in d.columns]\n",
    "\n",
    "print(\"Candidate features:\", candidate_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df99b993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq rows: 1000099 severity rows: 2788\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 — prepare datasets\n",
    "freq_df = d.copy()                       # full dataset – classification\n",
    "severity_df = d[d.get(\"ClaimEvent\",0) == 1].copy()  # only policies with claims\n",
    "\n",
    "print(\"freq rows:\", freq_df.shape[0], \"severity rows:\", severity_df.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b98d97dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric: ['vehicle_age', 'CustomValueEstimate', 'Cylinders', 'kilowatts', 'NumberOfDoors', 'NumberOfVehiclesInFleet', 'SumInsured', 'RegistrationYear']\n",
      "OneHot (low-card): ['make', 'VehicleType', 'Gender', 'Province', 'TermFrequency', 'AlarmImmobiliser', 'TrackingDevice', 'bodytype']\n",
      "Hasher (high-card): ['PostalCode']\n",
      "Preprocessor ready. Sections: ['num', 'ohe', 'hash']\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# FIXED MEMORY-SAFE PREPROCESSOR\n",
    "# -----------------------\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# numeric vs categorical split\n",
    "numeric_features = [c for c in candidate_features if freq_df[c].dtype.kind in \"biufc\"]\n",
    "categorical_candidates = [c for c in candidate_features if c not in numeric_features]\n",
    "\n",
    "# cardinality check\n",
    "cardinalities = {c: freq_df[c].nunique() for c in categorical_candidates}\n",
    "HASH_THRESHOLD = 100\n",
    "hash_features = [c for c,v in cardinalities.items() if v > HASH_THRESHOLD]\n",
    "ohe_features = [c for c in categorical_candidates if c not in hash_features]\n",
    "\n",
    "print(\"Numeric:\", numeric_features)\n",
    "print(\"OneHot (low-card):\", ohe_features)\n",
    "print(\"Hasher (high-card):\", hash_features)\n",
    "\n",
    "# numeric pipeline\n",
    "num_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=False)),  # sparse-friendly\n",
    "    (\"to_csr\", FunctionTransformer(lambda X: sp.csr_matrix(X), validate=False))\n",
    "])\n",
    "\n",
    "# one-hot pipeline\n",
    "cat_ohe_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True))\n",
    "]) if len(ohe_features) > 0 else (\"drop_ohe\", \"drop\", [])\n",
    "\n",
    "# --- FIXED HASHER PIPELINE ---\n",
    "def df_to_dict_list(X):\n",
    "    dfX = pd.DataFrame(X, columns=hash_features).astype(str)\n",
    "    dfX = dfX.replace(\"nan\", \"\")\n",
    "    return dfX.to_dict(orient=\"records\")\n",
    "\n",
    "hash_pipe = Pipeline([\n",
    "    (\"to_dict\", FunctionTransformer(df_to_dict_list, validate=False)),\n",
    "    (\"hasher\", FeatureHasher(n_features=128, input_type=\"dict\"))\n",
    "]) if len(hash_features) > 0 else (\"drop_hash\", \"drop\", [])\n",
    "\n",
    "# Build final sparse preprocessor\n",
    "transformers = []\n",
    "if len(numeric_features) > 0:\n",
    "    transformers.append((\"num\", num_pipe, numeric_features))\n",
    "if len(ohe_features) > 0:\n",
    "    transformers.append((\"ohe\", cat_ohe_pipe, ohe_features))\n",
    "if len(hash_features) > 0:\n",
    "    transformers.append((\"hash\", hash_pipe, hash_features))\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers, sparse_threshold=1.0)\n",
    "\n",
    "print(\"Preprocessor ready. Sections:\", [t[0] for t in transformers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06227f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 — Helper: safe preprocess function (returns scipy.sparse.csr_matrix)\n",
    "def safe_transform_preprocessor(prep, X_df):\n",
    "    \"\"\"\n",
    "    Fit/transform or transform X_df with preprocessor and ensure sparse CSR output.\n",
    "    Use fit_transform on training split and transform on others.\n",
    "    \"\"\"\n",
    "    Xt = prep.fit_transform(X_df) if not hasattr(prep, \"_is_fitted\") or not getattr(prep, \"_is_fitted\", False) else prep.transform(X_df)\n",
    "    # Mark as fitted\n",
    "    setattr(prep, \"_is_fitted\", True)\n",
    "    if sp.issparse(Xt):\n",
    "        return Xt.tocsr()\n",
    "    else:\n",
    "        # convert dense to CSR (should be small if only numeric)\n",
    "        return sp.csr_matrix(Xt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "929c6531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large dataset detected — stratified sampling to 300000 rows\n",
      "Fitting sparse preprocessor...\n",
      "Preprocessed shapes: (240000, 212) (60000, 212)\n",
      "Training Logistic Regression (saga, balanced)...\n",
      "Logistic Regression Metrics: {'roc_auc': 0.5585926737603295, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.9972166666666666}\n",
      "Training XGBoost on sparse DMatrix...\n",
      "XGBoost Metrics: {'roc_auc': 0.872477497497776, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'accuracy': 0.9971833333333333}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'logistic': {'roc_auc': 0.5585926737603295,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'accuracy': 0.9972166666666666},\n",
       " 'xgb': {'roc_auc': 0.872477497497776,\n",
       "  'precision': 0.0,\n",
       "  'recall': 0.0,\n",
       "  'f1': 0.0,\n",
       "  'accuracy': 0.9971833333333333}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 6 — Frequency modeling (sparse, memory-safe, final reviewed version)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score,\n",
    "    f1_score, accuracy_score\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 0. Optional: check XGBoost availability\n",
    "# -------------------------------\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except:\n",
    "    HAS_XGB = False\n",
    "    print(\"XGBoost not available — skipping boosted model.\")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Extract features and target\n",
    "# -------------------------------\n",
    "Xf = freq_df[candidate_features].copy()\n",
    "yf = freq_df[\"ClaimEvent\"].astype(int)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 2. PROTOTYPE-SAFE SAMPLING (STRATIFIED)\n",
    "# -------------------------------\n",
    "MAX_ROWS_PROTOTYPE = 300_000\n",
    "\n",
    "if Xf.shape[0] > MAX_ROWS_PROTOTYPE:\n",
    "    print(f\"Large dataset detected — stratified sampling to {MAX_ROWS_PROTOTYPE} rows\")\n",
    "\n",
    "    # stratified sample based on target balance\n",
    "    Xf_sample, _, yf_sample, _ = train_test_split(\n",
    "        Xf, yf,\n",
    "        train_size=MAX_ROWS_PROTOTYPE,\n",
    "        random_state=42,\n",
    "        stratify=yf\n",
    "    )\n",
    "    Xf, yf = Xf_sample, yf_sample\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Train/test split BEFORE preprocessing\n",
    "# -------------------------------\n",
    "Xf_train_raw, Xf_test_raw, yf_train, yf_test = train_test_split(\n",
    "    Xf, yf,\n",
    "    test_size=0.20,\n",
    "    random_state=42,\n",
    "    stratify=yf\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Fit preprocessor (sparse-safe)\n",
    "# -------------------------------\n",
    "print(\"Fitting sparse preprocessor...\")\n",
    "Xf_train = preprocessor.fit_transform(Xf_train_raw)\n",
    "\n",
    "if not sp.issparse(Xf_train):\n",
    "    Xf_train = sp.csr_matrix(Xf_train)\n",
    "\n",
    "Xf_test = preprocessor.transform(Xf_test_raw)\n",
    "if not sp.issparse(Xf_test):\n",
    "    Xf_test = sp.csr_matrix(Xf_test)\n",
    "\n",
    "print(\"Preprocessed shapes:\", Xf_train.shape, Xf_test.shape)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Logistic Regression (sparse-friendly)\n",
    "# -------------------------------\n",
    "print(\"Training Logistic Regression (saga, balanced)...\")\n",
    "\n",
    "log = LogisticRegression(\n",
    "    solver=\"saga\",\n",
    "    max_iter=400,\n",
    "    n_jobs=-1,\n",
    "    class_weight=\"balanced\"  # improves recall for rare claims\n",
    ")\n",
    "\n",
    "log.fit(Xf_train, yf_train)\n",
    "\n",
    "p_log = log.predict_proba(Xf_test)[:, 1]\n",
    "pred_log = log.predict(Xf_test)\n",
    "\n",
    "\n",
    "clf_results = {}\n",
    "clf_results[\"logistic\"] = {\n",
    "    \"roc_auc\": float(roc_auc_score(yf_test, p_log)),\n",
    "    \"precision\": float(precision_score(yf_test, pred_log, zero_division=0)),\n",
    "    \"recall\": float(recall_score(yf_test, pred_log, zero_division=0)),\n",
    "    \"f1\": float(f1_score(yf_test, pred_log, zero_division=0)),\n",
    "    \"accuracy\": float(accuracy_score(yf_test, pred_log))\n",
    "}\n",
    "\n",
    "print(\"Logistic Regression Metrics:\", clf_results[\"logistic\"])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 6. XGBoost (optional, only if installed)\n",
    "# -------------------------------\n",
    "if HAS_XGB:\n",
    "    print(\"Training XGBoost on sparse DMatrix...\")\n",
    "\n",
    "    dtrain = xgb.DMatrix(Xf_train, label=yf_train)\n",
    "    dtest = xgb.DMatrix(Xf_test, label=yf_test)\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"verbosity\": 0,\n",
    "        \"n_jobs\": -1\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.train(params, dtrain, num_boost_round=150)\n",
    "\n",
    "    p_xgb = xgb_model.predict(dtest)\n",
    "    pred_xgb = (p_xgb >= 0.5).astype(int)\n",
    "\n",
    "    clf_results[\"xgb\"] = {\n",
    "        \"roc_auc\": float(roc_auc_score(yf_test, p_xgb)),\n",
    "        \"precision\": float(precision_score(yf_test, pred_xgb, zero_division=0)),\n",
    "        \"recall\": float(recall_score(yf_test, pred_xgb, zero_division=0)),\n",
    "        \"f1\": float(f1_score(yf_test, pred_xgb, zero_division=0)),\n",
    "        \"accuracy\": float(accuracy_score(yf_test, pred_xgb))\n",
    "    }\n",
    "\n",
    "    print(\"XGBoost Metrics:\", clf_results[\"xgb\"])\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Final result dictionary\n",
    "# -------------------------------\n",
    "clf_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57279a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity metrics: {\n",
      "  \"rf\": {\n",
      "    \"rmse\": 37619.386421611336,\n",
      "    \"mae\": 16789.621904379484,\n",
      "    \"r2\": 0.12002486915087396\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 7 — Severity modeling (regression) memory-safe\n",
    "severity_metrics = {}\n",
    "\n",
    "if severity_df.shape[0] < 30:\n",
    "    print(\"Not enough claim rows to train severity models robustly.\")\n",
    "else:\n",
    "    Xs_raw = severity_df[candidate_features].copy()\n",
    "    ys = severity_df[\"TotalClaims\"].astype(float).fillna(0)\n",
    "\n",
    "    # --------------------\n",
    "    # IMPORTANT:\n",
    "    # Do NOT fit preprocessor again. Only transform.\n",
    "    # --------------------\n",
    "    Xs_proc = preprocessor.transform(Xs_raw)\n",
    "\n",
    "    # For small/medium dataset: attempt densify\n",
    "    if Xs_proc.shape[0] <= 200_000:\n",
    "        if sp.issparse(Xs_proc):\n",
    "            try:\n",
    "                Xs_dense = Xs_proc.toarray()\n",
    "            except MemoryError:\n",
    "                print(\"Too large to densify — using SGDRegressor on sparse matrix.\")\n",
    "                Xs_dense = None\n",
    "        else:\n",
    "            Xs_dense = Xs_proc\n",
    "\n",
    "        Xs_train, Xs_test, ys_train, ys_test = train_test_split(\n",
    "            Xs_dense if Xs_dense is not None else Xs_proc,\n",
    "            ys,\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        if Xs_dense is not None:\n",
    "            from sklearn.ensemble import RandomForestRegressor\n",
    "            rf_reg = RandomForestRegressor(n_estimators=150, n_jobs=-1, random_state=42)\n",
    "            rf_reg.fit(Xs_train, ys_train)\n",
    "            p_rf = rf_reg.predict(Xs_test)\n",
    "            severity_metrics[\"rf\"] = {\n",
    "                \"rmse\": float(np.sqrt(mean_squared_error(ys_test, p_rf))),\n",
    "                \"mae\": float(mean_absolute_error(ys_test, p_rf)),\n",
    "                \"r2\": float(r2_score(ys_test, p_rf))\n",
    "            }\n",
    "        else:\n",
    "            sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "            sgd.fit(Xs_train, ys_train)\n",
    "            p_sgd = sgd.predict(Xs_test)\n",
    "            severity_metrics[\"sgd\"] = {\n",
    "                \"rmse\": float(np.sqrt(mean_squared_error(ys_test, p_sgd))),\n",
    "                \"mae\": float(mean_absolute_error(ys_test, p_sgd)),\n",
    "                \"r2\": float(r2_score(ys_test, p_sgd))\n",
    "            }\n",
    "\n",
    "    else:\n",
    "        print(\"Severity too large — sampling 200k for training\")\n",
    "\n",
    "        sample_idx = severity_df.sample(n=200_000, random_state=42).index\n",
    "        Xs_sample = preprocessor.transform(severity_df.loc[sample_idx, candidate_features])\n",
    "        ys_sample = ys.loc[sample_idx]\n",
    "\n",
    "        sgd = SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "        sgd.fit(Xs_sample, ys_sample)\n",
    "\n",
    "        test_sample = severity_df.sample(n=20000, random_state=1) if severity_df.shape[0] > 20000 else severity_df\n",
    "        Xs_test_proc = preprocessor.transform(test_sample[candidate_features])\n",
    "        p_sgd = sgd.predict(Xs_test_proc)\n",
    "\n",
    "        severity_metrics[\"sgd\"] = {\n",
    "            \"rmse\": float(np.sqrt(mean_squared_error(test_sample[\"TotalClaims\"], p_sgd))),\n",
    "            \"mae\": float(mean_absolute_error(test_sample[\"TotalClaims\"], p_sgd)),\n",
    "            \"r2\": float(r2_score(test_sample[\"TotalClaims\"], p_sgd))\n",
    "        }\n",
    "\n",
    "    print(\"Severity metrics:\", json.dumps(severity_metrics, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "044c3bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predicted premiums: (1000099, 5)\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 — Risk-based premium and saving predictions\n",
    "# Choose frequency model scores and severity predictions:\n",
    "# Use logistic model (log) for p_claim; use rf_reg or sgd for severity predictions if available.\n",
    "\n",
    "# p_claim for full freq_df (do in chunks if too large)\n",
    "def predict_proba_chunked(model, preproc, X_df, chunk_size=100000):\n",
    "    \"\"\"Return probability vector for X_df using preproc.transform + model.predict_proba in chunks (memory-safe).\"\"\"\n",
    "    n = X_df.shape[0]\n",
    "    probs = np.zeros(n)\n",
    "    for start in range(0, n, chunk_size):\n",
    "        end = min(n, start+chunk_size)\n",
    "        X_chunk = preproc.transform(X_df.iloc[start:end])\n",
    "        if sp.issparse(X_chunk):\n",
    "            Xc = X_chunk\n",
    "        else:\n",
    "            Xc = sp.csr_matrix(X_chunk)\n",
    "        probs[start:end] = model.predict_proba(Xc)[:,1]\n",
    "    return probs\n",
    "\n",
    "# p_claim (we trained log on Xf_train etc.)\n",
    "# If preprocessor is fitted, use preprocessor.transform on the full Xf (do in chunks)\n",
    "p_claim_all = predict_proba_chunked(log, preprocessor, freq_df[candidate_features], chunk_size=200000)\n",
    "\n",
    "# severity prediction: depending on chosen severity model\n",
    "if \"rf\" in locals():\n",
    "    # rf_reg expects dense; transform in chunks and densify small chunks\n",
    "    def predict_sev_rf(rf_model, preproc, X_df, chunk_size=50000):\n",
    "        n = X_df.shape[0]\n",
    "        preds = np.zeros(n)\n",
    "        for start in range(0, n, chunk_size):\n",
    "            end = min(n, start+chunk_size)\n",
    "            Xc = preproc.transform(X_df.iloc[start:end])\n",
    "            if sp.issparse(Xc):\n",
    "                Xc = Xc.toarray()\n",
    "            preds[start:end] = rf_model.predict(Xc)\n",
    "        return preds\n",
    "    pred_sev_all = predict_sev_rf(rf_reg, preprocessor, freq_df[candidate_features])\n",
    "elif 'sgd' in locals():\n",
    "    # sgd accepts sparse\n",
    "    def predict_sev_sgd(sgd_model, preproc, X_df, chunk_size=200000):\n",
    "        n = X_df.shape[0]\n",
    "        preds = np.zeros(n)\n",
    "        for start in range(0, n, chunk_size):\n",
    "            end = min(n, start+chunk_size)\n",
    "            Xc = preproc.transform(X_df.iloc[start:end])\n",
    "            preds[start:end] = sgd_model.predict(Xc)\n",
    "        return preds\n",
    "    pred_sev_all = predict_sev_sgd(sgd, preprocessor, freq_df[candidate_features])\n",
    "else:\n",
    "    # fallback: use mean severity\n",
    "    mean_sev = severity_df[\"TotalClaims\"].mean() if severity_df.shape[0]>0 else 0.0\n",
    "    pred_sev_all = np.full(freq_df.shape[0], mean_sev)\n",
    "\n",
    "# Compute risk-based premium\n",
    "profit_margin = 0.10\n",
    "expense_loading = 100.0\n",
    "expected_loss = p_claim_all * pred_sev_all\n",
    "predicted_premium = expected_loss * (1 + profit_margin) + expense_loading\n",
    "\n",
    "out = freq_df[[\"PolicyID\"]].copy() if \"PolicyID\" in freq_df.columns else pd.DataFrame(index=freq_df.index)\n",
    "out[\"pred_prob_claim\"] = p_claim_all\n",
    "out[\"pred_severity\"] = pred_sev_all\n",
    "out[\"pred_risk_premium\"] = predicted_premium\n",
    "if \"CalculatedPremiumPerTerm\" in freq_df.columns:\n",
    "    out[\"true_premium\"] = freq_df[\"CalculatedPremiumPerTerm\"].values\n",
    "\n",
    "out.to_csv(\"reports/interim/predicted_premiums_memory_safe.csv\", index=False)\n",
    "print(\"Saved predicted premiums:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d93a5b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity feature importance error: name 'pipe_rf' is not defined\n",
      "Frequency feature importance error: name 'rf' is not defined\n",
      "SHAP failed: X has 212 features, but ColumnTransformer is expecting 17 features as input.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Feature importance + SHAP ===\n",
    "# build feature names after preprocessor fit\n",
    "num_names = numeric_features\n",
    "cat_names = []\n",
    "try:\n",
    "    ohe = preprocessor.named_transformers_['cat'].named_steps['ohe']\n",
    "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "except Exception:\n",
    "    cat_names = []\n",
    "feature_names = num_names + cat_names\n",
    "\n",
    "# RF severity importances\n",
    "try:\n",
    "    rf_reg = pipe_rf.named_steps['rf']\n",
    "    imp = rf_reg.feature_importances_\n",
    "    fi = pd.DataFrame({\"feature\": feature_names, \"importance\": imp}).sort_values(\"importance\", ascending=False).head(30)\n",
    "    fi.to_csv(\"reports/interim/severity_feature_importance.csv\", index=False)\n",
    "    display(fi.head(10))\n",
    "except Exception as e:\n",
    "    print(\"Severity feature importance error:\", e)\n",
    "\n",
    "# RF frequency importances\n",
    "try:\n",
    "    fi2 = pd.DataFrame({\"feature\": feature_names, \"importance\": rf.feature_importances_}).sort_values(\"importance\", ascending=False).head(30)\n",
    "    fi2.to_csv(\"reports/interim/freq_feature_importance.csv\", index=False)\n",
    "    display(fi2.head(10))\n",
    "except Exception as e:\n",
    "    print(\"Frequency feature importance error:\", e)\n",
    "\n",
    "# SHAP (optional)\n",
    "if HAS_SHAP:\n",
    "    try:\n",
    "        explainer = shap.TreeExplainer(rf_reg)\n",
    "        # use a sample of test rows transformed\n",
    "        Xs_sample = preprocessor.transform(Xs_test)\n",
    "        shap_vals = explainer.shap_values(Xs_sample)\n",
    "        shap.summary_plot(shap_vals, Xs_sample, feature_names=feature_names, show=False)\n",
    "        plt.savefig(\"reports/interim/figures/shap_severity_summary.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        # record mean abs shap\n",
    "        mean_abs = np.abs(shap_vals).mean(axis=0)\n",
    "        shap_df = pd.DataFrame({\"feature\":feature_names, \"mean_abs_shap\": mean_abs}).sort_values(\"mean_abs_shap\", ascending=False)\n",
    "        shap_df.head(10).to_csv(\"reports/interim/severity_shap_summary.csv\", index=False)\n",
    "        display(shap_df.head(10))\n",
    "    except Exception as e:\n",
    "        print(\"SHAP failed:\", e)\n",
    "else:\n",
    "    print(\"SHAP not installed; skip shap explanation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c09181a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estimate: each additional year of vehicle_age increases predicted claim amount by ~-1079.66 currency units (linear approximation).\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Top-5 human-readable interpretations & per-year effect example (FIXED) ===\n",
    "# If SHAP summary exists, produce sentences; otherwise use RF importances.\n",
    "\n",
    "shap_path = \"reports/severity_shap_summary.csv\"\n",
    "\n",
    "# ----- SHAP top features -----\n",
    "if os.path.exists(shap_path):\n",
    "    shap_t = pd.read_csv(shap_path).head(10)\n",
    "    for i, row in shap_t.head(5).iterrows():\n",
    "        print(\n",
    "            f\"- {i + 1}. {row['feature']}: mean|SHAP|={row['mean_abs_shap']:.3f} — \"\n",
    "            f\"higher values increase predicted claim amount on average.\"\n",
    "        )\n",
    "else:\n",
    "    fi_path = \"reports/severity_feature_importance.csv\"\n",
    "    if os.path.exists(fi_path):\n",
    "        fi = pd.read_csv(fi_path).head(10)\n",
    "        for i, row in fi.iterrows():\n",
    "            print(f\"- {i + 1}. {row['feature']}: importance={row['importance']:.4f}\")\n",
    "\n",
    "# ----- Per-year vehicle_age effect estimation -----\n",
    "# FIX: use rf_reg (trained model) instead of pipe_rf\n",
    "\n",
    "if (\n",
    "    \"vehicle_age\" in candidate_features\n",
    "    and \"Severity\" in severity_df.columns\n",
    "    and severity_df.shape[0] > 30\n",
    "):\n",
    "    try:\n",
    "        # Prepare raw features\n",
    "        Xs_raw = severity_df[candidate_features].copy()\n",
    "\n",
    "        # Preprocess features (rf_reg trained on dense preprocessed data)\n",
    "        Xs_proc = preprocessor.transform(Xs_raw)\n",
    "\n",
    "        if sp.issparse(Xs_proc):\n",
    "            Xs_proc = Xs_proc.toarray()\n",
    "\n",
    "        # Predict severity\n",
    "        sev_pred_all = rf_reg.predict(Xs_proc)\n",
    "\n",
    "        # Build temp df for regression\n",
    "        tmp = severity_df.copy()\n",
    "        tmp[\"sev_pred\"] = sev_pred_all\n",
    "        tmp = tmp[tmp[\"vehicle_age\"].notnull()]\n",
    "\n",
    "        if tmp.shape[0] > 20:\n",
    "            coef = np.polyfit(\n",
    "                tmp[\"vehicle_age\"].astype(float),\n",
    "                tmp[\"sev_pred\"].astype(float),\n",
    "                1\n",
    "            )[0]\n",
    "\n",
    "            print(\n",
    "                f\"\\nEstimate: each additional year of vehicle_age increases \"\n",
    "                f\"predicted claim amount by ~{coef:.2f} currency units \"\n",
    "                f\"(linear approximation).\"\n",
    "            )\n",
    "\n",
    "    except NameError:\n",
    "        print(\"\\nCannot perform vehicle_age effect estimation: Severity model (rf_reg) not defined.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nCannot perform vehicle_age effect estimation: An error occurred ({e}).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
