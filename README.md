# ğŸš— Insurance Portfolio Risk Analysis 

This repository contains the deliverables for the Insurance Risk Analytics project.  
The focus is to establish strong engineering foundations through Version Control, Exploratory Data Analysis (EDA), and reproducible data management with DVC.

---

## ğŸ“ Project Structure
```bash
.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Raw input data (tracked with DVC only)
â”‚ â””â”€â”€ processed/ # Cleaned and transformed datasets
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_eda.ipynb # Exploratory Data Analysis
â”‚ â”œâ”€â”€ 02_hypothesis_tests
â”‚ â”œâ”€â”€ 03_modeling_task4
â”œâ”€â”€ dvc.yaml # Pipeline definition
â”œâ”€â”€ plots/
â”œâ”€â”€ reports/
â”‚ â”œâ”€â”€ figures/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

```
---


### **Task 1 â€” Git + EDA**
- Repository initialized and structured.
- Branch `task-1` created, work committed with meaningful messages.
- Exploratory Data Analysis performed:
  - Data understanding (dtype, missing values, structure)
  - Descriptive statistics
  - Univariate & bivariate analysis
  - Outlier detection via boxplots
  - Geographical + temporal trends
- Creative visualizations produced:
  - Radar chart for Loss Ratio by Vehicle Type  
  - Heatmap: Province Ã— Vehicle Type  
  - Claim Frequency Area Chart

Notebook:  
ğŸ“„ `notebooks/01_eda.ipynb`

---

### **Task 2 â€” Reproducible Data Pipeline with DVC**
- DVC installed and initialized
- Local remote storage configured
- Raw insurance dataset added with `dvc add`
- Data versioning established (v1, v2)
- Data pushed to the local remote using `dvc push`
- Branch `task-2` created and merged with a PR

Commands used:

```bash
pip install dvc
dvc init
```

# Set up local remote
```bash
mkdir -p ../dvc_storage
dvc remote add -d localstorage ../dvc_storage
```

# Track data
```bash
dvc add data/raw/insurance_data.txt
git add data/raw/insurance_data.txt.dvc .gitignore
git commit -m "Track raw insurance dataset with DVC"
```

# Push data to remote
```bash
dvc push
```

ğŸš€ How to Reproduce the Pipeline
Clone the repository

Install requirements:

```bash
pip install -r requirements.txt
```
Pull data from DVC remote:
```bash
dvc pull
```
Open notebooks:
```bash
jupyter lab
```
### Task 3 â€” Hypothesis Testing & Statistical Evidence
âœ” Key Analyses Performed

- T-tests comparing claim amounts across:

    - Gender
    - Fuel type
    - Vehicle type

- Chi-Square tests:

    - ClaimEvent Ã— Region
    - ClaimEvent Ã— Gender

- ANOVA for multi-category comparisons
- Correlation significance tests for numeric predictors

âœ” Outcomes

- Clean summary table of all hypothesis tests
- File exported as:

    - hypothesis_test_summary.csv

ğŸ“„ Notebook: notebooks/02_hypothesis_tests.ipynb
ğŸ“„ Output: data/processed/hypothesis_test_summary.csv

### Task 4 â€” Predictive Modelling (Frequency, Severity, Pure Premium)
âœ” 1. Data Preprocessing

- Robust memory-safe transformations
- Numeric imputation
- Categorical encoding with OneHotEncoder
- Feature engineering:

    - VehicleAge
    - VehiclePowerRatio
    - DriverExperience
    - ClaimRateByProvince


âœ” 2. Frequency Modelling (Binary Classification)

Models used:
  - LogisticRegression(solver='saga') â€” sparse & scalable
  - Optional: XGBoost with DMatrix

Metrics:
  - ROC-AUC
  - F1
  - Precision
  - Recall
  - Accuracy

âœ” 3. Severity Modelling (Regression)

Models:
  - RandomForestRegressor
  - GradientBoostingRegressor
  - Median Benchmark

Metrics:
  - RMSE
  - MAE
  - RÂ²

âœ” 4. Pure Premium Estimation

For each policy:

Pure Premium = P(ClaimEvent=1) Ã— E[ClaimCost | ClaimEvent=1]


Computed using chunked memory-safe predictions:
  - predict_proba_chunked()
  - predict_chunked_regression()

Final output saved:
ğŸ“„ data/processed/pure_premium_predictions.csv

âœ” 5. Model Explainability 

  - SHAP values (TreeExplainer / LinearExplainer)
  - Feature impact visualization
  - Interpretation of primary drivers of risk


ğŸš€ Final Deliverables
| **Component** | **Description**                                      |
|---------------|------------------------------------------------------|
| **Task 1**    | EDA + visuals + repo structure                       |
| **Task 2**    | DVC pipeline + versioned datasets                    |
| **Task 3**    | Statistical hypothesis testing + summary CSV         |
| **Task 4**    | Frequency, severity, pure premium modeling + predictions CSV |


All tasks are complete and reproducible using the instructions below.

ğŸ› ï¸ How to Run Everything
```bash
git clone https://github.com/kalkidanzabreham/insurance-risk-analytics
pip install -r requirements.txt
dvc pull
jupyter lab
```

ğŸ“Œ Notes
Raw data is never committed to Git, only tracked via DVC.
To create a new data version:

```bash
dvc add data/raw/insurance_data_v2.txt
git commit -am "Add new dataset version"
dvc push
```
âœ¨ Author
Kalkidan Abreham â€” Data Science & Machine Learning
